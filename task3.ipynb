{"cells":[{"cell_type":"markdown","id":"1df29eda","metadata":{"id":"1df29eda"},"source":["Step 0. Unzip enron1.zip into the current directory."]},{"cell_type":"markdown","id":"bf32cfce","metadata":{"id":"bf32cfce"},"source":["Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."]},{"cell_type":"code","execution_count":7,"id":"20c5d195","metadata":{"id":"20c5d195"},"outputs":[{"name":"stdout","output_type":"stream","text":["skipped 2248.2004-09-23.GP.spam.txt\n","skipped 4201.2005-04-05.GP.spam.txt\n","skipped 2526.2004-10-17.GP.spam.txt\n","skipped 2698.2004-10-31.GP.spam.txt\n","skipped 4142.2005-03-31.GP.spam.txt\n","skipped 2140.2004-09-13.GP.spam.txt\n","skipped 4350.2005-04-23.GP.spam.txt\n","skipped 4566.2005-05-24.GP.spam.txt\n","skipped 5105.2005-08-31.GP.spam.txt\n","skipped 2042.2004-08-30.GP.spam.txt\n","skipped 1414.2004-06-24.GP.spam.txt\n","skipped 2649.2004-10-27.GP.spam.txt\n","skipped 0754.2004-04-01.GP.spam.txt\n","skipped 3364.2005-01-01.GP.spam.txt\n","skipped 3304.2004-12-26.GP.spam.txt\n"]}],"source":["import pandas as pd\n","import os\n","\n","def read_spam():\n","    category = 'spam'\n","    directory = './enron1/spam'\n","    return read_category(category, directory)\n","\n","def read_ham():\n","    category = 'ham'\n","    directory = './enron1/ham'\n","    return read_category(category, directory)\n","\n","def read_category(category, directory):\n","    emails = []\n","    for filename in os.listdir(directory):\n","        if not filename.endswith(\".txt\"):\n","            continue\n","        with open(os.path.join(directory, filename), 'r') as fp:\n","            try:\n","                content = fp.read()\n","                emails.append({'name': filename, 'content': content, 'category': category})\n","            except:\n","                print(f'skipped {filename}')\n","    return emails\n","\n","ham = read_ham()\n","spam = read_spam()\n","\n","df_ham = pd.DataFrame.from_records(ham)\n","df_spam = pd.DataFrame.from_records(spam)\n","\n","df = pd.concat([df_ham, df_spam], ignore_index=True)"]},{"cell_type":"code","execution_count":8,"id":"29891643","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>content</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0020.1999-12-15.farmer.ham.txt</td>\n","      <td>Subject: meter 1431 - nov 1999\\ndaren -\\ncould...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4496.2001-05-07.farmer.ham.txt</td>\n","      <td>Subject: tenaska\\ndarren ,\\nattached is the la...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3948.2001-03-22.farmer.ham.txt</td>\n","      <td>Subject: calpine daily gas nomination\\nstill u...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2288.2000-09-19.farmer.ham.txt</td>\n","      <td>Subject: cornhusker\\ndarren ,\\nhow are things ...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1105.2000-05-22.farmer.ham.txt</td>\n","      <td>Subject: re : pg &amp; e texas contract 5098 - 695...</td>\n","      <td>ham</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             name  \\\n","0  0020.1999-12-15.farmer.ham.txt   \n","1  4496.2001-05-07.farmer.ham.txt   \n","2  3948.2001-03-22.farmer.ham.txt   \n","3  2288.2000-09-19.farmer.ham.txt   \n","4  1105.2000-05-22.farmer.ham.txt   \n","\n","                                             content category  \n","0  Subject: meter 1431 - nov 1999\\ndaren -\\ncould...      ham  \n","1  Subject: tenaska\\ndarren ,\\nattached is the la...      ham  \n","2  Subject: calpine daily gas nomination\\nstill u...      ham  \n","3  Subject: cornhusker\\ndarren ,\\nhow are things ...      ham  \n","4  Subject: re : pg & e texas contract 5098 - 695...      ham  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","id":"1a1c23fd","metadata":{"id":"1a1c23fd"},"source":["Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."]},{"cell_type":"code","execution_count":9,"id":"c447c901","metadata":{"id":"c447c901"},"outputs":[],"source":["import re\n","\n","def preprocessor(e):\n","    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', e)\n","    cleaned_text = cleaned_text.lower()\n","    return cleaned_text"]},{"cell_type":"markdown","id":"ba32521d","metadata":{"id":"ba32521d"},"source":["Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."]},{"cell_type":"code","execution_count":11,"id":"1442d377","metadata":{"id":"1442d377"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.98\n","Confusion Matrix:\n","[[692  15]\n"," [  8 317]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         ham       0.99      0.98      0.98       707\n","        spam       0.95      0.98      0.96       325\n","\n","    accuracy                           0.98      1032\n","   macro avg       0.97      0.98      0.97      1032\n","weighted avg       0.98      0.98      0.98      1032\n","\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","data = pd.DataFrame({\n","    'content': df['content'],  \n","    'category': df['category']  \n","})\n","\n","vectorizer = CountVectorizer(preprocessor=preprocessor)\n","\n","X = data['content']\n","y = data['category']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","X_train_vectorized = vectorizer.fit_transform(X_train)\n","X_test_vectorized = vectorizer.transform(X_test)\n","\n","model = LogisticRegression()\n","model.fit(X_train_vectorized, y_train)\n","\n","y_pred = model.predict(X_test_vectorized)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","class_report = classification_report(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.2f}')\n","print('Confusion Matrix:')\n","print(conf_matrix)\n","print('Classification Report:')\n","print(class_report)"]},{"cell_type":"markdown","id":"9674d032","metadata":{"id":"9674d032"},"source":["Step 4."]},{"cell_type":"code","execution_count":13,"id":"6b7d78c9","metadata":{"id":"6b7d78c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 positive features (likely to be spam):\n","        feature  coefficient\n","17751      http     1.032538\n","28480    prices     0.889743\n","25152        no     0.851404\n","26597  paliourg     0.738187\n","30309   removed     0.727645\n","17060      here     0.709494\n","24035     money     0.668877\n","16994     hello     0.667583\n","18737      info     0.632774\n","23387   message     0.625233\n","\n","Top 10 negative features (likely to be ham):\n","        feature  coefficient\n","2543   attached    -1.750258\n","12615     enron    -1.538438\n","9456      daren    -1.448526\n","35559    thanks    -1.431525\n","11031       doc    -1.365414\n","27538  pictures    -1.201775\n","24855      neon    -1.166349\n","9650       deal    -1.117037\n","17672       hpl    -1.081072\n","32945    sitara    -1.025823\n"]}],"source":["feature_names = vectorizer.get_feature_names_out()\n","\n","coefficients = model.coef_[0]\n","\n","feature_importance = pd.DataFrame({\n","    'feature': feature_names,\n","    'coefficient': coefficients\n","})\n","\n","feature_importance['abs_coefficient'] = feature_importance['coefficient'].abs()\n","sorted_features = feature_importance.sort_values(by='abs_coefficient', ascending=False)\n","\n","top_positive_features = sorted_features[sorted_features['coefficient'] > 0].head(10)\n","print(\"Top 10 positive features (likely to be spam):\")\n","print(top_positive_features[['feature', 'coefficient']])\n","\n","top_negative_features = sorted_features[sorted_features['coefficient'] < 0].head(10)\n","print(\"\\nTop 10 negative features (likely to be ham):\")\n","print(top_negative_features[['feature', 'coefficient']])"]},{"cell_type":"markdown","id":"d267e7ad","metadata":{"id":"d267e7ad"},"source":["Submission\n","1. Upload the jupyter notebook to Forage."]},{"cell_type":"markdown","id":"LI4u_ZUGToDQ","metadata":{"id":"LI4u_ZUGToDQ"},"source":["All Done!"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"task3.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
